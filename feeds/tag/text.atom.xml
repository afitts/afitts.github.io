<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alex Fitts</title><link href="http://localhost:8000/" rel="alternate"></link><link href="http://localhost:8000/feeds/tag/text.atom.xml" rel="self"></link><id>http://localhost:8000/</id><updated>2015-03-30T00:00:00-05:00</updated><entry><title>Nonsensical beer reviews via Markov chains</title><link href="http://localhost:8000/2015/03/30/beer-review-markov-chains/" rel="alternate"></link><published>2015-03-30T00:00:00-05:00</published><author><name>Alex Fitts</name></author><id>tag:localhost:8000,2015-03-30:2015/03/30/beer-review-markov-chains/</id><summary type="html">&lt;p&gt;I’ve had a bunch of beer reviews and ratings data sitting on my hard drive for about year. For a beer nerd like me, that’s a pretty cool dataset, yet I’ve let it collect digital dust.&lt;/p&gt;
&lt;p&gt;Fast forward to last week, where somehow I wound up in the Wikipedia Death Spiral. You know what I mean - you click a link to a Wikipedia article, that article takes you to a new one, then you’re on another, and another … we’ve all been there. And it’s kind of awesome.&lt;/p&gt;
&lt;p&gt;Well, the rabbit hole led me to &lt;a href="http://en.wikipedia.org/wiki/Markov_chain"&gt;Markov chains&lt;/a&gt;, which seemed like a good excuse to mess around with that beer review data.&lt;/p&gt;
&lt;h2&gt;What are Markov chains?&lt;/h2&gt;
&lt;p&gt;Markov chains are a random process that transitions to various states, where the “next state” is based on its probability distribution, given the current state.&lt;/p&gt;
&lt;p&gt;Imagine we have the following sequence of days, where S indicates it was sunny and R indicates it was rainy:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;S S R R S R S S R R R R S R S S S R&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let’s pick a random beginning “state” - let’s just say it’s S (sunny). The next state is based &lt;strong&gt;only&lt;/strong&gt; on the current state. Since our current state is S, we only need to look at observations immediately following a sunny day.&lt;/p&gt;
&lt;p&gt;To illustrate, let’s look at the weather pattern again, this time putting the observations to be considered in bold.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;S &lt;strong&gt;S&lt;/strong&gt; &lt;strong&gt;R&lt;/strong&gt; R S &lt;strong&gt;R&lt;/strong&gt; S &lt;strong&gt;S&lt;/strong&gt; &lt;strong&gt;R&lt;/strong&gt; R R R S &lt;strong&gt;R&lt;/strong&gt; S &lt;strong&gt;S&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt; &lt;strong&gt;R&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even though there are 18 observations, only nine need to be considered for the possible next state. Of the nine, four are S and five are R, giving us a 44% (4/9) chance of the next state being sunny and a 55% (5/9) chance of it being rainy.&lt;/p&gt;
&lt;p&gt;Now, let’s assume our beginning state (S) transitioned to a second state of R (which it had a 55% chance of doing). Here are the states we need to consider for the possible third state:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;S S R &lt;strong&gt;R&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt; R &lt;strong&gt;S&lt;/strong&gt; S R &lt;strong&gt;R&lt;/strong&gt; &lt;strong&gt;R&lt;/strong&gt; &lt;strong&gt;R&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt; R &lt;strong&gt;S&lt;/strong&gt; S S R&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There’s an equal chance (4/8) the third state will be S or R.&lt;/p&gt;
&lt;p&gt;With a second-order Markov chain, the current state is two observations. Let’s assume a beginning state of SR and use the same weather sequence as above, again putting the possible next states in bold.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;S S R &lt;strong&gt;R&lt;/strong&gt; S R &lt;strong&gt;S&lt;/strong&gt; S R &lt;strong&gt;R&lt;/strong&gt; R R S R &lt;strong&gt;S&lt;/strong&gt; S S R&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This time there are only four observations to consider as possible “next states,” with an equal chance it’ll be S or R.&lt;/p&gt;
&lt;p&gt;Let’s assume the “next state” picked is R. Now our current (second) state is RR - the S from our beginning state is forgotten. The following are possible third states:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;S S R R &lt;strong&gt;S&lt;/strong&gt; R S S R R &lt;strong&gt;R&lt;/strong&gt; &lt;strong&gt;R&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt; R S S S R&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, there’s an equal chance of our third state being S or R.&lt;/p&gt;
&lt;p&gt;We can continue picking “next states” and eventually we’ll have generated a random, yet probabilistic sequence of weather.&lt;/p&gt;
&lt;p&gt;These same principles can be used to generate a sentence from text data - pick a random beginning state (word) from the text and then pick the next word based on the likelihood of it occurring, given the current word. A first-order Markov sentence would have a one word current state, a second-order would have a two word current state, … and so on.&lt;/p&gt;
&lt;p&gt;The larger the corpus and the higher the order, the more sense these Markov generated sentences make. Good thing I have a lot of beer reviews.&lt;/p&gt;
&lt;h2&gt;The (mini) project&lt;/h2&gt;
&lt;p&gt;This seemed ripe for a Twitter bot, so I created &lt;a href="https://twitter.com/BeerSnobSays"&gt;BeerSnobSays&lt;/a&gt;, which tweets nonsensical beer reviews generated via second-order Markov chains.&lt;/p&gt;
&lt;p&gt;Not everything it tweets makes much sense:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;dissipates about a finger of head and some mild spice interwoven and even beer at a local Greek restaurant.&lt;/p&gt;
&lt;p&gt;a big thumbs up though and there are plenty other choices that I was really no distinguishing characteristics that stand out.&lt;/p&gt;
&lt;p&gt;those who are looking for a beer best characteristic of this beer into the hype and the lager style that is unwelcome.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;But some of it is pretty funny:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;off by itself, the taste of apple juice colored brew with a nice warming alcohol bathes your noodle in its dryness.&lt;/p&gt;
&lt;p&gt;is almost like sour grains with a hint of booze in the finish, with sweet orange peels and pine sap.&lt;/p&gt;
&lt;p&gt;a charred woodiness and smoke can run into pineapple, oranges and citrusy oils with a clean alcohol sting at the bottom of the recipe.&lt;/p&gt;
&lt;p&gt;the berry aspect is evident but the tartness and dryness from the beer starts off surprisingly pleasant.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m not sure if that last one’s from the bot or a famous poet.&lt;/p&gt;
&lt;p&gt;You can &lt;a href="https://twitter.com/gjreda"&gt;follow me&lt;/a&gt; and &lt;a href="https://twitter.com/BeerSnobSays"&gt;BeerSnobSays&lt;/a&gt; on Twitter. You can also find the code for the bot &lt;a href="https://github.com/gjreda/beer-snob-says"&gt;on GitHub&lt;/a&gt;.&lt;/p&gt;</summary><category term="beer"></category><category term="text"></category><category term="markov chain"></category></entry></feed>