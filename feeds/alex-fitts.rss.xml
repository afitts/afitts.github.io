<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Alex Fitts - Alex Fitts</title><link>http://afitts.github.io/</link><description></description><lastBuildDate>Sun, 18 Nov 2018 00:00:00 -0600</lastBuildDate><item><title>Histopathologic Cancer Detection with New Fastai Lib</title><link>http://afitts.github.io/2018/11/18/cancer/</link><description>&lt;p&gt;We're going to tackle binary image classification with the newly released fastai-v1 library.&lt;/p&gt;
&lt;p&gt;First we turn on autoreload incase we update any of our modules while running the notebook. We also include the magic matplotlib inline command to make sure our plots stay in our notebook after we've run it …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Fitts</dc:creator><pubDate>Sun, 18 Nov 2018 00:00:00 -0600</pubDate><guid isPermaLink="false">tag:afitts.github.io,2018-11-18:/2018/11/18/cancer/</guid></item><item><title>Getting to know humpback whales with EDA</title><link>http://afitts.github.io/2018/11/11/humpback-eda/</link><description>&lt;p&gt;Today I wanted to try my hand at a kaggle &lt;a href="https://www.kaggle.com/c/humpback-whale-identification"&gt;competition&lt;/a&gt; that seemed like another great place to practice using image neural networks. The competition asks us to identify humpback whales from their flukes (tail fins). Before getting into the model training, however, it's always important to &lt;strong&gt;look at your …&lt;/strong&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Fitts</dc:creator><pubDate>Sun, 11 Nov 2018 00:00:00 -0600</pubDate><guid isPermaLink="false">tag:afitts.github.io,2018-11-11:/2018/11/11/humpback-eda/</guid></item><item><title>How I learned to stop worrying and love Random Forests (Part 1)</title><link>http://afitts.github.io/2018/10/12/rf_part1/</link><description>&lt;p&gt;In this notebook I'll show how a non-paramtric model like Random Forests can not only fit structured data incredibly well but can also be easily interpretable! To do this I'll review step-by-step the Random Forest model training workflow covered in the Fast.ai Machine Learning &lt;a href="https://course.fast.ai/ml"&gt;course&lt;/a&gt; (which I highly recommend …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Fitts</dc:creator><pubDate>Fri, 12 Oct 2018 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:afitts.github.io,2018-10-12:/2018/10/12/rf_part1/</guid></item><item><title>ISLR Resampling Methods Exercises</title><link>http://afitts.github.io/2016/10/01/islr-chap5/</link><description>&lt;p&gt;Keeping the streak going but now with exercises from chapter 5 in An Introduction to Statistical Learning with Applications in R.&lt;/p&gt;
&lt;h1&gt;5.&lt;/h1&gt;
&lt;p&gt;In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Fitts</dc:creator><pubDate>Sat, 01 Oct 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:afitts.github.io,2016-10-01:/2016/10/01/islr-chap5/</guid></item><item><title>ISLR Classification Exercises</title><link>http://afitts.github.io/2016/09/25/islr-chap4/</link><description>&lt;p&gt;New week new exercises! Here's my answers for chapter 4 in An Introduction to Statistical Learning with Applications in R.&lt;/p&gt;
&lt;h1&gt;4. The curse of dimensionality&lt;/h1&gt;
&lt;p&gt;When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Fitts</dc:creator><pubDate>Sun, 25 Sep 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:afitts.github.io,2016-09-25:/2016/09/25/islr-chap4/</guid></item><item><title>ISLR Linear Regression Exercises</title><link>http://afitts.github.io/2016/09/18/islr-chap3/</link><description>&lt;p&gt;Currently working on the exercises from chapter 3 in An Introduction to Statistical Learning with Applications in R.&lt;/p&gt;
&lt;h1&gt;1.&lt;/h1&gt;
&lt;p&gt;The small p values for TV and radio correspond to the low probability of observing the t statistics we see by chance. Hence TV and radio &lt;em&gt;do&lt;/em&gt; have a relationship with …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Fitts</dc:creator><pubDate>Sun, 18 Sep 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:afitts.github.io,2016-09-18:/2016/09/18/islr-chap3/</guid></item><item><title>Hello Universe</title><link>http://afitts.github.io/2016/06/10/hello-world/</link><description>&lt;p&gt;Hello UNIVERSE. This is Alex Fitts reporting in. &lt;/p&gt;
&lt;p&gt;This space will serve as a hub for all the data science and programming projects I plan to embark on. I may occaisionally dip into astronomy, as is my nature, but hopefully I can find a wonderful marriage between the two. But …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex Fitts</dc:creator><pubDate>Fri, 10 Jun 2016 00:00:00 -0500</pubDate><guid isPermaLink="false">tag:afitts.github.io,2016-06-10:/2016/06/10/hello-world/</guid></item></channel></rss>